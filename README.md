# LLM Cache

LLM Cache is a comprehensive set of caching strategies for AI models.

It includes:
- **Na√Øve Caching**: A simple caching mechanism that stores responses from AI models without any advanced features.
- **Coalescing Caching**: A more advanced caching strategy that groups similar queries together to reduce the number of requests sent to AI models, improving performance.
- **Semantic Caching**: A more sophisticated approach that uses vector search to find similar queries and cache their responses, improving retrieval efficiency.
- **Batched Caching**: A strategy that groups similar queries together to reduce the number of requests sent to AI models, further enhancing performance and reducing costs.

Curious about how it works? Read my [in-depth article on Medium](https://medium.com/@TomasZezula/llm-caching-strategies-from-na%C3%AFve-to-semantic-and-batched-6b5816e7488a).

# LLM Cache

LLM Cache is a comprehensive set of caching strategies for AI models. 

Benefits:
* Designed to improve performance and reduce costs by caching responses from AI models. 
* Supports both vector search and exact search, allowing for efficient retrieval of cached responses.

Curious about how it works? Read an in-depth article on TBD.
